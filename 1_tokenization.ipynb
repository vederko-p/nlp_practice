{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97bee85f-b62f-42a9-a9a7-4ffac782db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4b033-3947-430c-9e89-5a2b2206cf1e",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d675c8dd-c158-433b-83f9-43c0417613e6",
   "metadata": {},
   "source": [
    "1. [YT. Stanford CS336 (2025) Overview and Tokenization](https://www.youtube.com/watch?v=msHyYioAyNE&list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_&index=3)\n",
    "2. [Git. Stanford CS336 (2025) Assignment 1 - Basics](https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a38040-89cd-47b2-b9c4-74e021e05ef8",
   "metadata": {},
   "source": [
    "# 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c213571-00ee-46e1-9c8b-95f430e59eb0",
   "metadata": {},
   "source": [
    "## 1.1. GPT-2 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696378da-c19a-40da-a71c-b064c0b3f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compression_ratio(string: str, indices: list[int]) -> float:\n",
    "    \"\"\"Given `string` that has been tokenized into `indices`, calculate\n",
    "    how many bites are represented by a token.\"\"\"\n",
    "    num_bytes = len(bytes(string, encoding=\"utf-8\"))\n",
    "    num_tokens = len(indices)\n",
    "    return num_bytes / num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da10ced6-10cd-427c-9597-718255d3d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de3cf95-e3f4-4804-9b2b-db29100d849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, 游깴! 擔먼봏!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "676b8425-944b-41f8-a978-4ba01a7f62b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 12520, 234, 235, 0, 220, 19526, 254, 25001, 121, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize\n",
    "indices = tokenizer_gpt2.encode(text)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f07c243-8f01-41b3-ae79-4d04084826ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, 游깴! 擔먼봏!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reconstruct\n",
    "reconstructed_string = tokenizer_gpt2.decode(indices)\n",
    "reconstructed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b416ba60-c157-4035-9cff-34a735164b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6666666666666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compression ratio\n",
    "get_compression_ratio(text, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca6cfc5-19c2-489f-b2ac-118b10a71f6f",
   "metadata": {},
   "source": [
    "## 1.2. Character based tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "875c0636-5526-4994-8b5c-7a8c898e130c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c713f18f-36ea-4e87-b7c6-33125678e25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127757"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"游깴\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4df2b0a-f8a4-4cef-8127-17249b2c5caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6f8cb23-5962-4c1e-a351-83a3fec24a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'游깴'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(127757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fac34d8-7860-4fbc-9598-a395b3ec4335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTokenizer:\n",
    "    \"\"\"Represent a string as a sequence of Unicode code points.\"\"\"\n",
    "    \n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        return list(map(ord, string))\n",
    "        \n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        return \"\".join(map(chr, indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93089e11-3034-49d1-b2fa-c901c564dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_char = CharacterTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9422471b-e5f3-45f9-b85b-ba08929d2b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 101, 108, 108, 111, 44, 32, 127757, 33, 32, 20320, 22909, 33]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = tokenizer_char.encode(text)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64cee671-602f-482c-85fe-5aafdde0901b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, 游깴! 擔먼봏!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_string = tokenizer_char.decode(indices)\n",
    "reconstructed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a5caff0-784e-4735-9cc3-113486cbdc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5384615384615385"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_compression_ratio(text, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88813838-8fae-4f76-8398-37276105a8d2",
   "metadata": {},
   "source": [
    "## 1.3. Byte-Based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6867f1d1-9ec9-45c8-ad3a-6a95a10e2de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'a'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(\"a\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b53f9d40-87bb-4d87-84bd-2032504c70a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xf0\\x9f\\x8c\\x8d'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(\"游깴\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3b6fb30-1ef3-4f47-99b1-fe8c9c3c9178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteTokenizer:\n",
    "    \"\"\"Represent a string as a sequence of bytes.\"\"\"\n",
    "    \n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        string_bytes = string.encode(\"utf-8\")\n",
    "        indices = list(map(int, string_bytes))\n",
    "        return indices\n",
    "\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        string_bytes = bytes(indices)\n",
    "        string = string_bytes.decode(\"utf-8\")\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e50b6da-7c04-4815-b536-51a17d4f0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_byte = ByteTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dc24d70-4c12-4a3c-8ab1-7f657def235b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 44,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 140,\n",
       " 141,\n",
       " 33,\n",
       " 32,\n",
       " 228,\n",
       " 189,\n",
       " 160,\n",
       " 229,\n",
       " 165,\n",
       " 189,\n",
       " 33]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = tokenizer_byte.encode(text)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44544413-3b67-4498-98df-0cf990f76377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, 游깴! 擔먼봏!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_string = tokenizer_byte.decode(indices)\n",
    "reconstructed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d9e6893-190b-42f9-8963-187ad5535c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_compression_ratio(text, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d054dd-6a50-425f-839d-6d63ea82ce39",
   "metadata": {},
   "source": [
    "## 1.4. Word-Based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "233dee4a-cc06-4232-932a-4d04f992f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I'll say supercalifragilisticexpialidocious!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a0c155b-7a6b-412c-a898-ae69f4672668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'\", 'll', ' ', 'say', ' ', 'supercalifragilisticexpialidocious', '!']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments = re.findall(r\"\\w+|.\", text)\n",
    "segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b071213-c224-41c0-8c94-73b237cc5546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py#L23\n",
    "GPT2_TOKENIZER_REGEX = \\\n",
    "    r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9c6bf80f-12ac-4839-84d4-a69723d2a438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the', ' quick', ' brown', ' fox']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments = re.findall(GPT2_TOKENIZER_REGEX, text)\n",
    "segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf617916-312e-4917-845c-8946b4b9272e",
   "metadata": {},
   "source": [
    "## 1.5. Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3dd7b46-ee38-4823-9702-eaedc39255e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class BPETokenizerParams:\n",
    "    \"\"\"All you need to specify a BPETokenizer.\"\"\"\n",
    "    vocab: dict[int, bytes]             # index -> bytes\n",
    "    merges: dict[tuple[int, int], int]  # index1,index2 -> new_index\n",
    "\n",
    "\n",
    "def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:\n",
    "    \"\"\"Return `indices`, but with all instances of `pair` replaced with `new_index`.\"\"\"\n",
    "    new_indices = []\n",
    "    i = 0\n",
    "    while i < len(indices):\n",
    "        if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:\n",
    "            new_indices.append(new_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_indices.append(indices[i])\n",
    "            i += 1\n",
    "    return new_indices\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\"BPE tokenizer given a set of merges and a vocabulary.\"\"\"\n",
    "    \n",
    "    def __init__(self, params: BPETokenizerParams):\n",
    "        self.params = params\n",
    "        \n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        indices = list(map(int, string.encode(\"utf-8\")))\n",
    "        # Note: this is a very slow implementation\n",
    "        for pair, new_index in self.params.merges.items():\n",
    "            indices = merge(indices, pair, new_index)\n",
    "        return indices\n",
    "        \n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        bytes_list = list(map(self.params.vocab.get, indices))\n",
    "        string = b\"\".join(bytes_list).decode(\"utf-8\")\n",
    "        return string\n",
    "\n",
    "\n",
    "def train_bpe(string: str, num_merges: int) -> BPETokenizerParams:\n",
    "    # Start with the list of bytes of string.\n",
    "    indices = list(map(int, string.encode(\"utf-8\")))\n",
    "    merges: dict[tuple[int, int], int] = {}  # index1, index2 => merged index\n",
    "    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # index -> bytes\n",
    "    for i in range(num_merges):\n",
    "        # Count the number of occurrences of each pair of tokens\n",
    "        counts = defaultdict(int)\n",
    "        for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair\n",
    "            counts[(index1, index2)] += 1\n",
    "        # Find the most common pair.\n",
    "        pair = max(counts, key=counts.get)\n",
    "        index1, index2 = pair\n",
    "        # Merge that pair.\n",
    "        new_index = 256 + i\n",
    "        merges[pair] = new_index\n",
    "        vocab[new_index] = vocab[index1] + vocab[index2]\n",
    "        indices = merge(indices, pair, new_index)\n",
    "    return BPETokenizerParams(vocab=vocab, merges=merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3388309a-f69c-4ce8-965b-a4e8dbc19edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the tokenizer\n",
    "string = \"the cat in the hat\"\n",
    "params = train_bpe(string, num_merges=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d864137-1a03-4c69-bf04-bab55450d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bpe_valid = BPETokenizer(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2b62ba7-96a1-4efd-82c8-ce3fad8ce24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"the quick brown fox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa90ba4c-0ca5-4150-b66d-68cf4bd1a03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[258, 113, 117, 105, 99, 107, 32, 98, 114, 111, 119, 110, 32, 102, 111, 120]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = tokenizer_bpe_valid.encode(text)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a895301-343f-43ef-bdd1-4d963683bfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick brown fox'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_string = tokenizer_bpe_valid.decode(indices)\n",
    "reconstructed_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd19c81-6d49-4c64-b2d4-638681395ec1",
   "metadata": {},
   "source": [
    "# 2. BPE Implementation from Scratch\n",
    "\n",
    "CS336 Assignment 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a196d6cf-894f-4e1f-b975-4ff3732a92b3",
   "metadata": {},
   "source": [
    "Goals:\n",
    "\n",
    "1) `encode()` currently loops over all merges. Only loop over merges that matter.\n",
    "2) Detect and preserve special tokens (e.g., `<|endoftext|>`).\n",
    "3) Use pre-tokenization (e.g., the GPT-2 tokenizer regex).\n",
    "4) Try to make the implementation as fast as possible.\n",
    "\n",
    "---\n",
    "\n",
    "Problem (train_bpe): BPE Tokenizer Training (15 points)\n",
    "\n",
    "**Deliverable**: Write a function that, given a path to an input text file, trains a (byte-level) BPE tokenizer. Your BPE training function should handle (at least) the following input parameters:\n",
    "\n",
    "|Parameter|Typing|Functionality|\n",
    "|:-|:-|:-|\n",
    "| `input_path`|`str` (Path)| Path to a text file containing BPE tokenizer training data.|\n",
    "| `vocab_size`|`int`| A positive integer defining the maximum final vocabulary size (includes initial byte vocabulary, merged items, and special tokens).|\n",
    "|`special_tokens`|`list[str]`|List of strings to add to the vocabulary (these tokens don't affect BPE training).|\n",
    "\n",
    "Your BPE training function should return the resulting vocabulary and merges:\n",
    "\n",
    "| Parameter | Typing | Functionality |\n",
    "|:-|:-|:-|\n",
    "| `vocab` | `dict[int, bytes]` | The tokenizer vocabulary, a mapping from `int` (token ID in the vocabulary) to `bytes` (token bytes). |\n",
    "| `merges` | `list[tuple[bytes, bytes]]` | A list of BPE merges produced from training. Each list item is a tuple of bytes `(<token1>, <token2>)`, representing that `<token1>` was merged with `<token2>`. The merges should be ordered by order of creation. |\n",
    "\n",
    "To test your BPE training function against our provided tests, you will first need to implement the test adapter at `[adapters.run_train_bpe]`. Then, run `uv run pytest tests/test_train_bpe.py`. Your implementation should be able to pass all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ed1f910-a24f-4b78-a342-a2129a4d0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_text = \"the quick brown fox\"\n",
    "sample_text = \"the cat in the hat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f0b8b23-d2a5-4677-a4bb-5320bff4f318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the string into a sequence of bytes using UTF-8 encoding\n",
    "# each character may be 1+ bytes long\n",
    "sample_text.encode(\"utf-8\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2b68025-ce4f-4248-8132-045b14589555",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_bytes = list(map(int, sample_text.encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c73c67d0-f293-4de8-8403-3b5e40c20cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bytes(text_bytes).decode('utf-8') == text_bytes  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5eb36989-ee0a-4a2b-8459-70ea8f8ddb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class BPETokenizerParams:\n",
    "    \"\"\"All you need to specify a BPETokenizer.\"\"\"\n",
    "    vocab: dict[int, bytes]             # index -> bytes\n",
    "    merges: dict[tuple[int, int], int]  # index1,index2 -> new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "377f8028-508e-40b7-ab3a-f9e7f72498ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:\n",
    "    indices_merged = []\n",
    "    pair_idx = 0\n",
    "    while pair_idx != len(indices) - 1:\n",
    "        old_pair = (indices[pair_idx], indices[pair_idx+1])\n",
    "        if old_pair == pair:\n",
    "            indices_merged.append(new_index)\n",
    "            pair_idx += 2\n",
    "        else:\n",
    "            indices_merged.append(indices[pair_idx])\n",
    "            pair_idx += 1\n",
    "    indices_merged.append(indices[-1])\n",
    "    return indices_merged\n",
    "\n",
    "\n",
    "def train_bpe(text: str, num_merges: int) -> BPETokenizerParams:\n",
    "    indices = list(map(int, text.encode(\"utf-8\")))\n",
    "    merges = {}\n",
    "    vocab = {x: bytes([x]) for x in range(256)}\n",
    "    for i in range(num_merges):\n",
    "        counts = {}\n",
    "        for idx_1, idx_2 in zip(indices, indices[1:]):\n",
    "            counts[(idx_1, idx_2)] = counts.get((idx_1, idx_2), 0) + 1\n",
    "        pair = max(counts, key=counts.get)\n",
    "        new_idx = 256 + i\n",
    "        merges[pair] = new_idx\n",
    "        indices = merge(indices, pair, new_idx)\n",
    "        vocab[new_idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "    return BPETokenizerParams(vocab, merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a977e82-c6d6-4dc1-b061-f3dd628f724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = train_bpe(sample_text, num_merges=3)\n",
    "# train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "752d1e2c-bebe-45f7-874e-5c78126de543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizerCustom:\n",
    "    \"\"\"BPE tokenizer given a set of merges and a vocabulary.\"\"\"\n",
    "    \n",
    "    def __init__(self, params: BPETokenizerParams):\n",
    "        self.params = params\n",
    "    \n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        indices = list(map(int, string.encode(\"utf-8\")))\n",
    "        for merge_pair, merge_idx in self.params.merges.items():\n",
    "            indices = merge(indices, merge_pair, merge_idx)\n",
    "        return indices\n",
    "        \n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        bytes_list = [self.params.vocab[idx] for idx in indices]\n",
    "        string = b\"\".join(bytes_list).decode(\"utf-8\")\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54d85c1e-8823-4877-8759-15aaee57eb52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[258, 99, 97, 116, 32, 105, 110, 32, 258, 104, 97, 116]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bpe_custom = BPETokenizerCustom(train_res)\n",
    "_t_encoded = tokenizer_bpe_custom.encode(sample_text)\n",
    "_t_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5173b511-60fb-475d-8c49-c3fd7f49917f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_t_encoded == tokenizer_bpe_valid.encode(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d83e1eda-e939-4bc0-a355-3c8784d7c254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat in the hat'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bpe_custom.decode(_t_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deaff3f-58fe-4a79-8c4a-110c38d4734c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e24119cd-0578-46eb-86f2-0d994962cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b38141e5-9c71-4855-b07d-e06f1830f107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['some', ' text', ' that', ' i', \"'ll\", ' pre', '-', 'tokenize']"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(PAT, \"some text that i'll pre-tokenize\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c50b99-c70a-47a3-a37a-54f1c830ce19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
