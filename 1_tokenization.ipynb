{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97bee85f-b62f-42a9-a9a7-4ffac782db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4b033-3947-430c-9e89-5a2b2206cf1e",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d675c8dd-c158-433b-83f9-43c0417613e6",
   "metadata": {},
   "source": [
    "1. [YT. Stanford CS336 (2025) Overview and Tokenization](https://www.youtube.com/watch?v=msHyYioAyNE&list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_&index=3)\n",
    "2. [Git. Stanford CS336 (2025) Assignment 1 - Basics](https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a38040-89cd-47b2-b9c4-74e021e05ef8",
   "metadata": {},
   "source": [
    "# 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c213571-00ee-46e1-9c8b-95f430e59eb0",
   "metadata": {},
   "source": [
    "## 1.1. GPT-2 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696378da-c19a-40da-a71c-b064c0b3f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compression_ratio(string: str, indices: list[int]) -> float:\n",
    "    \"\"\"Given `string` that has been tokenized into `indices`, calculate\n",
    "    how many bites are represented by a token.\"\"\"\n",
    "    num_bytes = len(bytes(string, encoding=\"utf-8\"))\n",
    "    num_tokens = len(indices)\n",
    "    return num_bytes / num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da10ced6-10cd-427c-9597-718255d3d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de3cf95-e3f4-4804-9b2b-db29100d849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, 游깴! 擔먼봏!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "676b8425-944b-41f8-a978-4ba01a7f62b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 12520, 234, 235, 0, 220, 19526, 254, 25001, 121, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize\n",
    "indices = tokenizer_gpt2.encode(text)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f07c243-8f01-41b3-ae79-4d04084826ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, 游깴! 擔먼봏!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reconstruct\n",
    "reconstructed_string = tokenizer_gpt2.decode(indices)\n",
    "reconstructed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b416ba60-c157-4035-9cff-34a735164b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6666666666666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compression ratio\n",
    "get_compression_ratio(text, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca6cfc5-19c2-489f-b2ac-118b10a71f6f",
   "metadata": {},
   "source": [
    "## 1.2. Character based tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "875c0636-5526-4994-8b5c-7a8c898e130c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c713f18f-36ea-4e87-b7c6-33125678e25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127757"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"游깴\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4df2b0a-f8a4-4cef-8127-17249b2c5caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6f8cb23-5962-4c1e-a351-83a3fec24a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'游깴'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(127757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fac34d8-7860-4fbc-9598-a395b3ec4335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTokenizer:\n",
    "    \"\"\"Represent a string as a sequence of Unicode code points.\"\"\"\n",
    "    \n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        return list(map(ord, string))\n",
    "        \n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        return \"\".join(map(chr, indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93089e11-3034-49d1-b2fa-c901c564dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_char = CharacterTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9422471b-e5f3-45f9-b85b-ba08929d2b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 101, 108, 108, 111, 44, 32, 127757, 33, 32, 20320, 22909, 33]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = tokenizer_char.encode(text)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64cee671-602f-482c-85fe-5aafdde0901b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, 游깴! 擔먼봏!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_string = tokenizer_char.decode(indices)\n",
    "reconstructed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a5caff0-784e-4735-9cc3-113486cbdc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5384615384615385"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_compression_ratio(text, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88813838-8fae-4f76-8398-37276105a8d2",
   "metadata": {},
   "source": [
    "## 1.3. Byte-Based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6867f1d1-9ec9-45c8-ad3a-6a95a10e2de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'a'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(\"a\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b53f9d40-87bb-4d87-84bd-2032504c70a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xf0\\x9f\\x8c\\x8d'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(\"游깴\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3b6fb30-1ef3-4f47-99b1-fe8c9c3c9178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteTokenizer:\n",
    "    \"\"\"Represent a string as a sequence of bytes.\"\"\"\n",
    "    \n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        string_bytes = string.encode(\"utf-8\")\n",
    "        indices = list(map(int, string_bytes))\n",
    "        return indices\n",
    "\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        string_bytes = bytes(indices)\n",
    "        string = string_bytes.decode(\"utf-8\")\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e50b6da-7c04-4815-b536-51a17d4f0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_byte = ByteTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dc24d70-4c12-4a3c-8ab1-7f657def235b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 44,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 140,\n",
       " 141,\n",
       " 33,\n",
       " 32,\n",
       " 228,\n",
       " 189,\n",
       " 160,\n",
       " 229,\n",
       " 165,\n",
       " 189,\n",
       " 33]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = tokenizer_byte.encode(text)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44544413-3b67-4498-98df-0cf990f76377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, 游깴! 擔먼봏!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_string = tokenizer_byte.decode(indices)\n",
    "reconstructed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d9e6893-190b-42f9-8963-187ad5535c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_compression_ratio(text, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d054dd-6a50-425f-839d-6d63ea82ce39",
   "metadata": {},
   "source": [
    "## 1.4. Word-Based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "233dee4a-cc06-4232-932a-4d04f992f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I'll say supercalifragilisticexpialidocious!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a0c155b-7a6b-412c-a898-ae69f4672668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'\", 'll', ' ', 'say', ' ', 'supercalifragilisticexpialidocious', '!']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments = re.findall(r\"\\w+|.\", text)\n",
    "segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b071213-c224-41c0-8c94-73b237cc5546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py#L23\n",
    "GPT2_TOKENIZER_REGEX = \\\n",
    "    r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c6bf80f-12ac-4839-84d4-a69723d2a438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'ll\", ' say', ' supercalifragilisticexpialidocious', '!']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments = re.findall(GPT2_TOKENIZER_REGEX, text)\n",
    "segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf617916-312e-4917-845c-8946b4b9272e",
   "metadata": {},
   "source": [
    "## 1.5. Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3dd7b46-ee38-4823-9702-eaedc39255e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class BPETokenizerParams:\n",
    "    \"\"\"All you need to specify a BPETokenizer.\"\"\"\n",
    "    vocab: dict[int, bytes]             # index -> bytes\n",
    "    merges: dict[tuple[int, int], int]  # index1,index2 -> new_index\n",
    "\n",
    "\n",
    "def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:\n",
    "    \"\"\"Return `indices`, but with all instances of `pair` replaced with `new_index`.\"\"\"\n",
    "    new_indices = []\n",
    "    i = 0\n",
    "    while i < len(indices):\n",
    "        if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:\n",
    "            new_indices.append(new_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_indices.append(indices[i])\n",
    "            i += 1\n",
    "    return new_indices\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\"BPE tokenizer given a set of merges and a vocabulary.\"\"\"\n",
    "    \n",
    "    def __init__(self, params: BPETokenizerParams):\n",
    "        self.params = params\n",
    "        \n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        indices = list(map(int, string.encode(\"utf-8\")))\n",
    "        # Note: this is a very slow implementation\n",
    "        for pair, new_index in self.params.merges.items():\n",
    "            indices = merge(indices, pair, new_index)\n",
    "        return indices\n",
    "        \n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        bytes_list = list(map(self.params.vocab.get, indices))\n",
    "        string = b\"\".join(bytes_list).decode(\"utf-8\")\n",
    "        return string\n",
    "\n",
    "\n",
    "def train_bpe(string: str, num_merges: int) -> BPETokenizerParams:\n",
    "    # Start with the list of bytes of string.\n",
    "    indices = list(map(int, string.encode(\"utf-8\")))\n",
    "    merges: dict[tuple[int, int], int] = {}  # index1, index2 => merged index\n",
    "    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # index -> bytes\n",
    "    for i in range(num_merges):\n",
    "        # Count the number of occurrences of each pair of tokens\n",
    "        counts = defaultdict(int)\n",
    "        for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair\n",
    "            counts[(index1, index2)] += 1\n",
    "        # Find the most common pair.\n",
    "        pair = max(counts, key=counts.get)\n",
    "        index1, index2 = pair\n",
    "        # Merge that pair.\n",
    "        new_index = 256 + i\n",
    "        merges[pair] = new_index\n",
    "        vocab[new_index] = vocab[index1] + vocab[index2]\n",
    "        indices = merge(indices, pair, new_index)\n",
    "    return BPETokenizerParams(vocab=vocab, merges=merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3388309a-f69c-4ce8-965b-a4e8dbc19edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the tokenizer\n",
    "string = \"the cat in the hat\"\n",
    "params = train_bpe(string, num_merges=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d864137-1a03-4c69-bf04-bab55450d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bpe_valid = BPETokenizer(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2b62ba7-96a1-4efd-82c8-ce3fad8ce24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"the quick brown fox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa90ba4c-0ca5-4150-b66d-68cf4bd1a03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[258, 113, 117, 105, 99, 107, 32, 98, 114, 111, 119, 110, 32, 102, 111, 120]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = tokenizer_bpe_valid.encode(text)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a895301-343f-43ef-bdd1-4d963683bfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick brown fox'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_string = tokenizer_bpe_valid.decode(indices)\n",
    "reconstructed_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd19c81-6d49-4c64-b2d4-638681395ec1",
   "metadata": {},
   "source": [
    "# 2. BPE Implementation from Scratch\n",
    "\n",
    "CS336 Assignment 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a196d6cf-894f-4e1f-b975-4ff3732a92b3",
   "metadata": {},
   "source": [
    "Goals:\n",
    "\n",
    "1) `encode()` currently loops over all merges. Only loop over merges that matter.\n",
    "2) Detect and preserve special tokens (e.g., `<|endoftext|>`).\n",
    "3) Use pre-tokenization (e.g., the GPT-2 tokenizer regex).\n",
    "4) Try to make the implementation as fast as possible.\n",
    "\n",
    "You are free to use the starter code at the following link verbatim to obtain chunk boundaries, which you can then use to distribute work across your processes:\n",
    "\n",
    "https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py\n",
    "\n",
    "---\n",
    "\n",
    "Problem (train_bpe): BPE Tokenizer Training (15 points)\n",
    "\n",
    "**Deliverable**: Write a function that, given a path to an input text file, trains a (byte-level) BPE tokenizer. Your BPE training function should handle (at least) the following input parameters:\n",
    "\n",
    "|Parameter|Typing|Functionality|\n",
    "|:-|:-|:-|\n",
    "| `input_path`|`str` (Path)| Path to a text file containing BPE tokenizer training data.|\n",
    "| `vocab_size`|`int`| A positive integer defining the maximum final vocabulary size (includes initial byte vocabulary, merged items, and special tokens).|\n",
    "|`special_tokens`|`list[str]`|List of strings to add to the vocabulary (these tokens don't affect BPE training).|\n",
    "\n",
    "Your BPE training function should return the resulting vocabulary and merges:\n",
    "\n",
    "| Parameter | Typing | Functionality |\n",
    "|:-|:-|:-|\n",
    "| `vocab` | `dict[int, bytes]` | The tokenizer vocabulary, a mapping from `int` (token ID in the vocabulary) to `bytes` (token bytes). |\n",
    "| `merges` | `list[tuple[bytes, bytes]]` | A list of BPE merges produced from training. Each list item is a tuple of bytes `(<token1>, <token2>)`, representing that `<token1>` was merged with `<token2>`. The merges should be ordered by order of creation. |\n",
    "\n",
    "To test your BPE training function against our provided tests, you will first need to implement the test adapter at `[adapters.run_train_bpe]`. Then, run `uv run pytest tests/test_train_bpe.py`. Your implementation should be able to pass all tests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee41d4-c927-49d3-9d70-1236ceabcd26",
   "metadata": {},
   "source": [
    "## 2.1. Vocabulary initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3d4273b-e56b-4ee9-bbf1-56565122d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = ['<|endoftext|>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6837becc-63d5-4b60-92c4-f0573ec629a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vocabulary(special_tokens: list[str]) -> dict[int, bytes]:\n",
    "    vocab = {x: bytes([x]) for x in range(256)}\n",
    "    for spec_tok in special_tokens:\n",
    "        vocab[len(vocab)] = spec_tok.encode(\"utf-8\")\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7bb5f05-9fd2-4d64-9e7c-138215ccf6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = init_vocabulary(SPECIAL_TOKENS)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a316c40d-8175-4e2b-aa17-7bb3c7da2413",
   "metadata": {},
   "source": [
    "## 2.2. Pre-Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f76617d-3559-433a-979e-35897f447e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "\n",
    "from aux.stanford_cs336.basics.pretokenization_example import find_chunk_boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8ead14a2-1746-4854-b980-7ec1402929ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_tokens(text: str, tokens: list[str]) -> str:\n",
    "    # Create a regex pattern that matches all keys\n",
    "    replacements = {tok: \"\" for tok in tokens}\n",
    "    pattern = re.compile(\"|\".join(map(re.escape, replacements.keys())))\n",
    "    # Use a lambda to replace each match with its corresponding value\n",
    "    return pattern.sub(lambda m: replacements[m.group(0)], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c0027ce-6556-403c-ad6a-1ab64fc21757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str, pattern: str) -> dict[bytes, int]:\n",
    "    token_count = {}\n",
    "    for match in re.finditer(pattern, text):\n",
    "        token = match.group()\n",
    "        token_bytes = token.encode(\"utf-8\")\n",
    "        token_count[token_bytes] = token_count.get(token_bytes, 0) + 1\n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "73dade72-3d36-4c02-b39b-481956346806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize(text: str, special_tokens: list[str], pretoken_pat: str) -> dict[tuple[bytes], int]:\n",
    "    \"\"\"Return bytes counts after special tokens removal and pre-tokenization.\"\"\"\n",
    "    text_clear = remove_special_tokens(text, special_tokens)\n",
    "    token_count = count_tokens(text_clear, pretoken_pat)\n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ad37bbbc-e052-4de2-afbb-03a080806d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PreTokenizerArgs:\n",
    "    n_proc: int\n",
    "    token_split: str\n",
    "    special_tokens: list[str]\n",
    "    pretoken_pat: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c7969fd1-db30-4c0f-808f-db7df71f42b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_file_parallel(filep: str, pt_args: PreTokenizerArgs) -> dict[tuple[bytes], int]:\n",
    "    with open(filep, \"rb\") as file:\n",
    "        bounds = find_chunk_boundaries(file, pt_args.n_proc, pt_args.token_split)\n",
    "        # Create arguments for each chunk\n",
    "        args = []\n",
    "        for beg, end in zip(bounds[:-1], bounds[1:]):\n",
    "            file.seek(beg)\n",
    "            chunk = file.read(end - beg).decode(\"utf-8\", errors=\"ignore\")\n",
    "            args.append((chunk, pt_args.special_tokens, pt_args.pretoken_pat))\n",
    "    # Process chunks in parallel\n",
    "    with Pool(processes=N_PROC) as pool:\n",
    "        results = pool.starmap(pretokenize, args)\n",
    "    # Reduce reults\n",
    "    pretoken_res = {}\n",
    "    for chunk_res in results:\n",
    "        for token_bytes, token_count in chunk_res.items():\n",
    "            pretoken_res[token_bytes] = pretoken_res.get(token_bytes, 0) + token_count\n",
    "    return pretoken_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "07d65b0b-a5ff-40c8-9571-4f0eb50d06f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_P = '/mnt/data/DatasetsML/NLP/natural_language_corpus/tiny_stories'\n",
    "\n",
    "TRAIN_P = os.path.join(DATA_ROOT_P, 'TinyStoriesV2-GPT4-train.txt')\n",
    "VALID_P = os.path.join(DATA_ROOT_P, 'TinyStoriesV2-GPT4-valid.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "816f2997-90ca-451b-b02b-63fd867b828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PROC = 8\n",
    "TOKEN_SPLIT = \"<|endoftext|>\".encode(\"utf-8\")\n",
    "SPECIAL_TOKENS = ['<|endoftext|>']\n",
    "PRETOKEN_PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c6bf45fa-5586-4b0a-8506-21dcc7ec3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tok_params = PreTokenizerArgs(\n",
    "    N_PROC,\n",
    "    TOKEN_SPLIT,\n",
    "    SPECIAL_TOKENS,\n",
    "    PRETOKEN_PAT,\n",
    ")\n",
    "_t_res_parallel = pretokenize_file_parallel(VALID_P, pre_tok_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7f4b43d-2372-4632-9e92-83d4d1e682a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_t_res_sub = {\n",
    "    k: _t_res_parallel[k]\n",
    "    for k, v in sorted(_t_res_parallel.items(), reverse=True, key=lambda x: x[1])[:30]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb90d5c9-a262-477e-8691-799ade22f772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'.': 421616,\n",
       " b',': 235432,\n",
       " b' the': 211031,\n",
       " b' and': 196057,\n",
       " b' a': 152161,\n",
       " b' to': 150493,\n",
       " b'\\n': 139288,\n",
       " b' was': 108019,\n",
       " b' They': 52425,\n",
       " b' it': 51670,\n",
       " b' He': 49241,\n",
       " b' \"': 47784,\n",
       " b' The': 46977,\n",
       " b' said': 43900,\n",
       " b' day': 43230,\n",
       " b' with': 42981,\n",
       " b' her': 38925,\n",
       " b' his': 38766,\n",
       " b' in': 38658,\n",
       " b' She': 38040,\n",
       " b' Tim': 37647,\n",
       " b' big': 35022,\n",
       " b' he': 32790,\n",
       " b' they': 29903,\n",
       " b' had': 28997,\n",
       " b' you': 28401,\n",
       " b' not': 27019,\n",
       " b' happy': 25863,\n",
       " b' on': 25720,\n",
       " b' of': 25467}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_t_res_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ffcfae-9c90-41fb-bcae-fae1eb78229c",
   "metadata": {},
   "source": [
    "## 2.3. BPE Merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8832ff54-e87f-4871-8dbb-1f69e0409794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a284cdce-2398-4356-bc8f-472d94f32718",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c205aa-11b1-4480-99d7-4d8fb714c628",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs_counter = {}\n",
    "    for token_bytes, token_inc in token_count.items():\n",
    "        pairs = zip(token_bytes, token_bytes[1:])\n",
    "        for pair in pairs:\n",
    "            pairs_counter[pair] = pairs_counter.get(pair, 0) + token_inc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "428a361c-e63d-400e-83ee-df327e96edec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(226, 128, 157, 46): 7,\n",
       " (226, 128, 157, 44): 1,\n",
       " (226, 128, 157): 4,\n",
       " (226, 128, 156): 19,\n",
       " (226, 128, 153): 122,\n",
       " (226, 128, 148): 4,\n",
       " (121, 101, 97, 114, 115): 1,\n",
       " (121, 101, 97, 114): 9,\n",
       " (119, 104, 111): 2,\n",
       " (119, 104, 97, 116): 3}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_t_res_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "726e3d5d-baa6-4dfb-8440-4f9f077da226",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vn'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# decode\n",
    "b\"\".join(map(vocabulary.get, (118, 110))).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a959a04-7369-4b97-bf7c-246bea1b861f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99493fea-6d92-44f6-a958-5db21b2c84ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ed1f910-a24f-4b78-a342-a2129a4d0969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_text = \"the quick brown fox\"\n",
    "sample_text = \"the cat in the hat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f0b8b23-d2a5-4677-a4bb-5320bff4f318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert the string into a sequence of bytes using UTF-8 encoding\n",
    "# each character may be 1+ bytes long\n",
    "sample_text.encode(\"utf-8\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2b68025-ce4f-4248-8132-045b14589555",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_bytes = list(map(int, sample_text.encode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c73c67d0-f293-4de8-8403-3b5e40c20cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bytes(text_bytes).decode('utf-8') == text_bytes  # True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5eb36989-ee0a-4a2b-8459-70ea8f8ddb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class BPETokenizerParams:\n",
    "    \"\"\"All you need to specify a BPETokenizer.\"\"\"\n",
    "    vocab: dict[int, bytes]             # index -> bytes\n",
    "    merges: dict[tuple[int, int], int]  # index1,index2 -> new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "377f8028-508e-40b7-ab3a-f9e7f72498ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:\n",
    "    indices_merged = []\n",
    "    pair_idx = 0\n",
    "    while pair_idx != len(indices) - 1:\n",
    "        old_pair = (indices[pair_idx], indices[pair_idx+1])\n",
    "        if old_pair == pair:\n",
    "            indices_merged.append(new_index)\n",
    "            pair_idx += 2\n",
    "        else:\n",
    "            indices_merged.append(indices[pair_idx])\n",
    "            pair_idx += 1\n",
    "    indices_merged.append(indices[-1])\n",
    "    return indices_merged\n",
    "\n",
    "\n",
    "def train_bpe(text: str, num_merges: int) -> BPETokenizerParams:\n",
    "    indices = list(map(int, text.encode(\"utf-8\")))\n",
    "    merges = {}\n",
    "    vocab = {x: bytes([x]) for x in range(256)}\n",
    "    for i in range(num_merges):\n",
    "        counts = {}\n",
    "        for idx_1, idx_2 in zip(indices, indices[1:]):\n",
    "            counts[(idx_1, idx_2)] = counts.get((idx_1, idx_2), 0) + 1\n",
    "        pair = max(counts, key=counts.get)\n",
    "        new_idx = 256 + i\n",
    "        merges[pair] = new_idx\n",
    "        indices = merge(indices, pair, new_idx)\n",
    "        vocab[new_idx] = vocab[pair[0]] + vocab[pair[1]]\n",
    "    return BPETokenizerParams(vocab, merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a977e82-c6d6-4dc1-b061-f3dd628f724a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_res = train_bpe(sample_text, num_merges=3)\n",
    "# train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "752d1e2c-bebe-45f7-874e-5c78126de543",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizerCustom:\n",
    "    \"\"\"BPE tokenizer given a set of merges and a vocabulary.\"\"\"\n",
    "    \n",
    "    def __init__(self, params: BPETokenizerParams):\n",
    "        self.params = params\n",
    "    \n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        indices = list(map(int, string.encode(\"utf-8\")))\n",
    "        for merge_pair, merge_idx in self.params.merges.items():\n",
    "            indices = merge(indices, merge_pair, merge_idx)\n",
    "        return indices\n",
    "        \n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        bytes_list = [self.params.vocab[idx] for idx in indices]\n",
    "        string = b\"\".join(bytes_list).decode(\"utf-8\")\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "54d85c1e-8823-4877-8759-15aaee57eb52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[258, 99, 97, 116, 32, 105, 110, 32, 258, 104, 97, 116]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bpe_custom = BPETokenizerCustom(train_res)\n",
    "_t_encoded = tokenizer_bpe_custom.encode(sample_text)\n",
    "_t_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5173b511-60fb-475d-8c49-c3fd7f49917f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_t_encoded == tokenizer_bpe_valid.encode(sample_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d83e1eda-e939-4bc0-a355-3c8784d7c254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat in the hat'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_bpe_custom.decode(_t_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deaff3f-58fe-4a79-8c4a-110c38d4734c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e24119cd-0578-46eb-86f2-0d994962cee9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38141e5-9c71-4855-b07d-e06f1830f107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c50b99-c70a-47a3-a37a-54f1c830ce19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
