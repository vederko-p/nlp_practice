{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97bee85f-b62f-42a9-a9a7-4ffac782db20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "from typing import Iterator, Any\n",
    "from multiprocessing import Pool\n",
    "from dataclasses import dataclass\n",
    "from collections import defaultdict\n",
    "\n",
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "import tokenization.bpe as custom_bpe\n",
    "import tokenization.pretokenization as custom_pretok\n",
    "\n",
    "from utils import timer\n",
    "from aux.stanford_cs336.basics.pretokenization_example import find_chunk_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be4b033-3947-430c-9e89-5a2b2206cf1e",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d675c8dd-c158-433b-83f9-43c0417613e6",
   "metadata": {},
   "source": [
    "1. [YT. Stanford CS336 (2025) Overview and Tokenization](https://www.youtube.com/watch?v=msHyYioAyNE&list=PLoROMvodv4rOY23Y0BoGoBGgQ1zmU_MT_&index=3)\n",
    "2. [Git. Stanford CS336 (2025) Assignment 1 - Basics](https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_spring2025_assignment1_basics.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a38040-89cd-47b2-b9c4-74e021e05ef8",
   "metadata": {},
   "source": [
    "# 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c213571-00ee-46e1-9c8b-95f430e59eb0",
   "metadata": {},
   "source": [
    "## 1.1. GPT-2 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696378da-c19a-40da-a71c-b064c0b3f8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_compression_ratio(string: str, indices: list[int]) -> float:\n",
    "    \"\"\"Given `string` that has been tokenized into `indices`, calculate\n",
    "    how many bites are represented by a token.\"\"\"\n",
    "    num_bytes = len(bytes(string, encoding=\"utf-8\"))\n",
    "    num_tokens = len(indices)\n",
    "    return num_bytes / num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da10ced6-10cd-427c-9597-718255d3d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_gpt2 = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6de3cf95-e3f4-4804-9b2b-db29100d849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Hello, 游깴! 擔먼봏!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "676b8425-944b-41f8-a978-4ba01a7f62b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496, 11, 12520, 234, 235, 0, 220, 19526, 254, 25001, 121, 0]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize\n",
    "indices = tokenizer_gpt2.encode(text)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f07c243-8f01-41b3-ae79-4d04084826ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, 游깴! 擔먼봏!'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reconstruct\n",
    "reconstructed_string = tokenizer_gpt2.decode(indices)\n",
    "reconstructed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b416ba60-c157-4035-9cff-34a735164b4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6666666666666667"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compression ratio\n",
    "get_compression_ratio(text, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca6cfc5-19c2-489f-b2ac-118b10a71f6f",
   "metadata": {},
   "source": [
    "## 1.2. Character based tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "875c0636-5526-4994-8b5c-7a8c898e130c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "97"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c713f18f-36ea-4e87-b7c6-33125678e25a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "127757"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ord(\"游깴\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4df2b0a-f8a4-4cef-8127-17249b2c5caf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6f8cb23-5962-4c1e-a351-83a3fec24a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'游깴'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chr(127757)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8fac34d8-7860-4fbc-9598-a395b3ec4335",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTokenizer:\n",
    "    \"\"\"Represent a string as a sequence of Unicode code points.\"\"\"\n",
    "    \n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        return list(map(ord, string))\n",
    "        \n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        return \"\".join(map(chr, indices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "93089e11-3034-49d1-b2fa-c901c564dd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_char = CharacterTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9422471b-e5f3-45f9-b85b-ba08929d2b60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72, 101, 108, 108, 111, 44, 32, 127757, 33, 32, 20320, 22909, 33]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = tokenizer_char.encode(text)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64cee671-602f-482c-85fe-5aafdde0901b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, 游깴! 擔먼봏!'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_string = tokenizer_char.decode(indices)\n",
    "reconstructed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a5caff0-784e-4735-9cc3-113486cbdc1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5384615384615385"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_compression_ratio(text, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88813838-8fae-4f76-8398-37276105a8d2",
   "metadata": {},
   "source": [
    "## 1.3. Byte-Based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6867f1d1-9ec9-45c8-ad3a-6a95a10e2de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'a'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(\"a\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b53f9d40-87bb-4d87-84bd-2032504c70a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'\\xf0\\x9f\\x8c\\x8d'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bytes(\"游깴\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f3b6fb30-1ef3-4f47-99b1-fe8c9c3c9178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteTokenizer:\n",
    "    \"\"\"Represent a string as a sequence of bytes.\"\"\"\n",
    "    \n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        string_bytes = string.encode(\"utf-8\")\n",
    "        indices = list(map(int, string_bytes))\n",
    "        return indices\n",
    "\n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        string_bytes = bytes(indices)\n",
    "        string = string_bytes.decode(\"utf-8\")\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e50b6da-7c04-4815-b536-51a17d4f0e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_byte = ByteTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7dc24d70-4c12-4a3c-8ab1-7f657def235b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[72,\n",
       " 101,\n",
       " 108,\n",
       " 108,\n",
       " 111,\n",
       " 44,\n",
       " 32,\n",
       " 240,\n",
       " 159,\n",
       " 140,\n",
       " 141,\n",
       " 33,\n",
       " 32,\n",
       " 228,\n",
       " 189,\n",
       " 160,\n",
       " 229,\n",
       " 165,\n",
       " 189,\n",
       " 33]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = tokenizer_byte.encode(text)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "44544413-3b67-4498-98df-0cf990f76377",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, 游깴! 擔먼봏!'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_string = tokenizer_byte.decode(indices)\n",
    "reconstructed_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6d9e6893-190b-42f9-8963-187ad5535c56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_compression_ratio(text, indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d054dd-6a50-425f-839d-6d63ea82ce39",
   "metadata": {},
   "source": [
    "## 1.4. Word-Based Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "233dee4a-cc06-4232-932a-4d04f992f0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"I'll say supercalifragilisticexpialidocious!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a0c155b-7a6b-412c-a898-ae69f4672668",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'\", 'll', ' ', 'say', ' ', 'supercalifragilisticexpialidocious', '!']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments = re.findall(r\"\\w+|.\", text)\n",
    "segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5b071213-c224-41c0-8c94-73b237cc5546",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/openai/tiktoken/blob/main/tiktoken_ext/openai_public.py#L23\n",
    "GPT2_TOKENIZER_REGEX = \\\n",
    "    r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c6bf80f-12ac-4839-84d4-a69723d2a438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'ll\", ' say', ' supercalifragilisticexpialidocious', '!']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segments = re.findall(GPT2_TOKENIZER_REGEX, text)\n",
    "segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf617916-312e-4917-845c-8946b4b9272e",
   "metadata": {},
   "source": [
    "## 1.5. Byte Pair Encoding (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b3dd7b46-ee38-4823-9702-eaedc39255e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class BPETokenizerParams:\n",
    "    \"\"\"All you need to specify a BPETokenizer.\"\"\"\n",
    "    vocab: dict[int, bytes]             # index -> bytes\n",
    "    merges: dict[tuple[int, int], int]  # index1,index2 -> new_index\n",
    "\n",
    "\n",
    "def merge(indices: list[int], pair: tuple[int, int], new_index: int) -> list[int]:\n",
    "    \"\"\"Return `indices`, but with all instances of `pair` replaced with `new_index`.\"\"\"\n",
    "    new_indices = []\n",
    "    i = 0\n",
    "    while i < len(indices):\n",
    "        if i + 1 < len(indices) and indices[i] == pair[0] and indices[i + 1] == pair[1]:\n",
    "            new_indices.append(new_index)\n",
    "            i += 2\n",
    "        else:\n",
    "            new_indices.append(indices[i])\n",
    "            i += 1\n",
    "    return new_indices\n",
    "\n",
    "\n",
    "class BPETokenizer:\n",
    "    \"\"\"BPE tokenizer given a set of merges and a vocabulary.\"\"\"\n",
    "    \n",
    "    def __init__(self, params: BPETokenizerParams):\n",
    "        self.params = params\n",
    "        \n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        indices = list(map(int, string.encode(\"utf-8\")))\n",
    "        # Note: this is a very slow implementation\n",
    "        for pair, new_index in self.params.merges.items():\n",
    "            indices = merge(indices, pair, new_index)\n",
    "        return indices\n",
    "        \n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        bytes_list = list(map(self.params.vocab.get, indices))\n",
    "        string = b\"\".join(bytes_list).decode(\"utf-8\")\n",
    "        return string\n",
    "\n",
    "\n",
    "def train_bpe(string: str, num_merges: int) -> BPETokenizerParams:\n",
    "    # Start with the list of bytes of string.\n",
    "    indices = list(map(int, string.encode(\"utf-8\")))\n",
    "    merges: dict[tuple[int, int], int] = {}  # index1, index2 => merged index\n",
    "    vocab: dict[int, bytes] = {x: bytes([x]) for x in range(256)}  # index -> bytes\n",
    "    for i in range(num_merges):\n",
    "        # Count the number of occurrences of each pair of tokens\n",
    "        counts = defaultdict(int)\n",
    "        for index1, index2 in zip(indices, indices[1:]):  # For each adjacent pair\n",
    "            counts[(index1, index2)] += 1\n",
    "        # Find the most common pair.\n",
    "        pair = max(counts, key=counts.get)\n",
    "        index1, index2 = pair\n",
    "        # Merge that pair.\n",
    "        new_index = 256 + i\n",
    "        merges[pair] = new_index\n",
    "        vocab[new_index] = vocab[index1] + vocab[index2]\n",
    "        indices = merge(indices, pair, new_index)\n",
    "    return BPETokenizerParams(vocab=vocab, merges=merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3388309a-f69c-4ce8-965b-a4e8dbc19edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the tokenizer\n",
    "string = \"the cat in the hat\"\n",
    "params = train_bpe(string, num_merges=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3d864137-1a03-4c69-bf04-bab55450d0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_bpe_valid = BPETokenizer(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2b62ba7-96a1-4efd-82c8-ce3fad8ce24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"the quick brown fox\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aa90ba4c-0ca5-4150-b66d-68cf4bd1a03b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[258, 113, 117, 105, 99, 107, 32, 98, 114, 111, 119, 110, 32, 102, 111, 120]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = tokenizer_bpe_valid.encode(text)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9a895301-343f-43ef-bdd1-4d963683bfc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the quick brown fox'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_string = tokenizer_bpe_valid.decode(indices)\n",
    "reconstructed_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd19c81-6d49-4c64-b2d4-638681395ec1",
   "metadata": {},
   "source": [
    "# 2. BPE Implementation from Scratch\n",
    "\n",
    "CS336 Assignment 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a196d6cf-894f-4e1f-b975-4ff3732a92b3",
   "metadata": {},
   "source": [
    "Goals:\n",
    "\n",
    "1) `encode()` currently loops over all merges. Only loop over merges that matter.\n",
    "2) Detect and preserve special tokens (e.g., `<|endoftext|>`).\n",
    "3) Use pre-tokenization (e.g., the GPT-2 tokenizer regex).\n",
    "4) Try to make the implementation as fast as possible.\n",
    "\n",
    "You are free to use the starter code at the following link verbatim to obtain chunk boundaries, which you can then use to distribute work across your processes:\n",
    "\n",
    "https://github.com/stanford-cs336/assignment1-basics/blob/main/cs336_basics/pretokenization_example.py\n",
    "\n",
    "---\n",
    "\n",
    "#### Problem (train_bpe): BPE Tokenizer Training (15 points)\n",
    "\n",
    "**Deliverable**: Write a function that, given a path to an input text file, trains a (byte-level) BPE tokenizer. Your BPE training function should handle (at least) the following input parameters:\n",
    "\n",
    "|Parameter|Typing|Functionality|\n",
    "|:-|:-|:-|\n",
    "| `input_path`|`str` (Path)| Path to a text file containing BPE tokenizer training data.|\n",
    "| `vocab_size`|`int`| A positive integer defining the maximum final vocabulary size (includes initial byte vocabulary, merged items, and special tokens).|\n",
    "|`special_tokens`|`list[str]`|List of strings to add to the vocabulary (these tokens don't affect BPE training).|\n",
    "\n",
    "Your BPE training function should return the resulting vocabulary and merges:\n",
    "\n",
    "| Parameter | Typing | Functionality |\n",
    "|:-|:-|:-|\n",
    "| `vocab` | `dict[int, bytes]` | The tokenizer vocabulary, a mapping from `int` (token ID in the vocabulary) to `bytes` (token bytes). |\n",
    "| `merges` | `list[tuple[bytes, bytes]]` | A list of BPE merges produced from training. Each list item is a tuple of bytes `(<token1>, <token2>)`, representing that `<token1>` was merged with `<token2>`. The merges should be ordered by order of creation. |\n",
    "\n",
    "To test your BPE training function against our provided tests, you will first need to implement the test adapter at `[adapters.run_train_bpe]`. Then, run `uv run pytest tests/test_train_bpe.py`. Your implementation should be able to pass all tests.\n",
    "\n",
    "---\n",
    "\n",
    "#### Problem (tokenizer): implementing the tokenizer (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ee41d4-c927-49d3-9d70-1236ceabcd26",
   "metadata": {},
   "source": [
    "## 2.1. Vocabulary initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b3d4273b-e56b-4ee9-bbf1-56565122d082",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPECIAL_TOKENS = ['<|endoftext|>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6837becc-63d5-4b60-92c4-f0573ec629a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_vocabulary(special_tokens: list[str]) -> dict[int, bytes]:\n",
    "    vocab = {x: bytes([x]) for x in range(256)}\n",
    "    for spec_tok in special_tokens:\n",
    "        vocab[len(vocab)] = spec_tok.encode(\"utf-8\")\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f7bb5f05-9fd2-4d64-9e7c-138215ccf6ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = init_vocabulary(SPECIAL_TOKENS)\n",
    "len(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a316c40d-8175-4e2b-aa17-7bb3c7da2413",
   "metadata": {},
   "source": [
    "## 2.2. Pre-Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8ead14a2-1746-4854-b980-7ec1402929ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_tokens(text: str, tokens: list[str]) -> str:\n",
    "    # Create a regex pattern that matches all keys\n",
    "    replacements = {tok: \"\" for tok in tokens}\n",
    "    pattern = re.compile(\"|\".join(map(re.escape, replacements.keys())))\n",
    "    # Use a lambda to replace each match with its corresponding value\n",
    "    return pattern.sub(lambda m: replacements[m.group(0)], text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6c0027ce-6556-403c-ad6a-1ab64fc21757",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tokens(text: str, pattern: str) -> dict[bytes, int]:\n",
    "    token_count = {}\n",
    "    for match in re.finditer(pattern, text):\n",
    "        token = match.group()\n",
    "        token_bytes = tuple(token.encode(\"utf-8\"))\n",
    "        token_count[token_bytes] = token_count.get(token_bytes, 0) + 1\n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "73dade72-3d36-4c02-b39b-481956346806",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize(text: str, special_tokens: list[str], pretoken_pat: str) -> dict[tuple[bytes], int]:\n",
    "    \"\"\"Return bytes counts after special tokens removal and pre-tokenization.\"\"\"\n",
    "    text_clear = remove_special_tokens(text, special_tokens)\n",
    "    token_count = count_tokens(text_clear, pretoken_pat)  # frequency table\n",
    "    return token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad37bbbc-e052-4de2-afbb-03a080806d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PreTokenizerArgs:\n",
    "    n_proc: int\n",
    "    token_split: str\n",
    "    special_tokens: list[str]\n",
    "    pretoken_pat: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c7969fd1-db30-4c0f-808f-db7df71f42b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_file_parallel(filep: str, pt_args: PreTokenizerArgs) -> dict[tuple[bytes], int]:\n",
    "    with open(filep, \"rb\") as file:\n",
    "        bounds = find_chunk_boundaries(file, pt_args.n_proc, pt_args.token_split)\n",
    "        # Create arguments for each chunk\n",
    "        args = []\n",
    "        for beg, end in zip(bounds[:-1], bounds[1:]):\n",
    "            file.seek(beg)\n",
    "            chunk = file.read(end - beg).decode(\"utf-8\", errors=\"ignore\")\n",
    "            args.append((chunk, pt_args.special_tokens, pt_args.pretoken_pat))\n",
    "    # Process chunks in parallel\n",
    "    with Pool(processes=N_PROC) as pool:\n",
    "        results = pool.starmap(pretokenize, args)\n",
    "    # Reduce results\n",
    "    pretoken_res = {}  # frequency table\n",
    "    for chunk_res in results:\n",
    "        for token_bytes, token_count in chunk_res.items():\n",
    "            pretoken_res[token_bytes] = pretoken_res.get(token_bytes, 0) + token_count\n",
    "    return pretoken_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "07d65b0b-a5ff-40c8-9571-4f0eb50d06f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_P = '/mnt/data/DatasetsML/NLP/natural_language_corpus/tiny_stories'\n",
    "\n",
    "TRAIN_P = os.path.join(DATA_ROOT_P, 'TinyStoriesV2-GPT4-train.txt')\n",
    "VALID_P = os.path.join(DATA_ROOT_P, 'TinyStoriesV2-GPT4-valid.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "816f2997-90ca-451b-b02b-63fd867b828f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PROC = 8\n",
    "TOKEN_SPLIT = \"<|endoftext|>\".encode(\"utf-8\")\n",
    "SPECIAL_TOKENS = ['<|endoftext|>']\n",
    "PRETOKEN_PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c6bf45fa-5586-4b0a-8506-21dcc7ec3d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_tok_params = PreTokenizerArgs(\n",
    "    N_PROC,\n",
    "    TOKEN_SPLIT,\n",
    "    SPECIAL_TOKENS,\n",
    "    PRETOKEN_PAT,\n",
    ")\n",
    "_t_res_parallel = pretokenize_file_parallel(VALID_P, pre_tok_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7f4b43d-2372-4632-9e92-83d4d1e682a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "_t_res_sub = {\n",
    "    bytes(k): _t_res_parallel[k]\n",
    "    for k, v in sorted(_t_res_parallel.items(), reverse=True, key=lambda x: x[1])[:30]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb90d5c9-a262-477e-8691-799ade22f772",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'.': 421616,\n",
       " b',': 235432,\n",
       " b' the': 211031,\n",
       " b' and': 196057,\n",
       " b' a': 152161,\n",
       " b' to': 150493,\n",
       " b'\\n': 139288,\n",
       " b' was': 108019,\n",
       " b' They': 52425,\n",
       " b' it': 51670,\n",
       " b' He': 49241,\n",
       " b' \"': 47784,\n",
       " b' The': 46977,\n",
       " b' said': 43900,\n",
       " b' day': 43230,\n",
       " b' with': 42981,\n",
       " b' her': 38925,\n",
       " b' his': 38766,\n",
       " b' in': 38658,\n",
       " b' She': 38040,\n",
       " b' Tim': 37647,\n",
       " b' big': 35022,\n",
       " b' he': 32790,\n",
       " b' they': 29903,\n",
       " b' had': 28997,\n",
       " b' you': 28401,\n",
       " b' not': 27019,\n",
       " b' happy': 25863,\n",
       " b' on': 25720,\n",
       " b' of': 25467}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_t_res_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ffcfae-9c90-41fb-bcae-fae1eb78229c",
   "metadata": {},
   "source": [
    "## 2.3. BPE Merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4bcbae46-48bb-4b1b-93e7-76a15c341113",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "low low low low low\n",
    "lower lower widest widest widest\n",
    "newest newest newest newest newest newest\n",
    "\"\"\".replace('\\n', ' ').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "08b1d698-f6d1-4363-a12d-fa998b72baed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretokenize_dummy(text: str) -> dict[tuple[bytes], int]:\n",
    "    freq_table = {}\n",
    "    for token in text.split(' '):\n",
    "        token_bytes = tuple(token.encode('utf-8'))\n",
    "        freq_table[token_bytes] = freq_table.get(token_bytes, 0) + 1\n",
    "    return freq_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f4b77c16-158f-493d-86b1-50bb5c0f11bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(108, 111, 119): 5,\n",
       " (108, 111, 119, 101, 114): 2,\n",
       " (119, 105, 100, 101, 115, 116): 3,\n",
       " (110, 101, 119, 101, 115, 116): 6}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_res_dummy = pretokenize_dummy(sample_text)\n",
    "t_res_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "67394b58-1fa5-45a3-bd24-18f0ad47c81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'low': 5, b'lower': 2, b'widest': 3, b'newest': 6}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_res_dummy_sub = {bytes(k): v for k, v in t_res_dummy.items()}\n",
    "t_res_dummy_sub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceb83f2-05ad-4c9d-82a4-114c7cd58207",
   "metadata": {},
   "source": [
    "The idea of optimization:\n",
    "\n",
    "1. Store pair to token map for each pair.\n",
    "2. Update the only pair count that overlaped with merged pair in updated token.\n",
    "\n",
    "Algorithm steps:\n",
    "\n",
    "1. Precompute initial pairs counts\n",
    "2. Precompute initial pair to token map\n",
    "3. In cycle:\n",
    "    - find the best pair for merge (`pair_counts`)\n",
    "    - update merges and vocab (`pair_locations`)\n",
    "    - for each affected token (all tokens that contain pairs to merge):\n",
    "        - merge and obtain new token\n",
    "        - update frequency table with new token, remove old one\n",
    "        - update pair counts for pairs that overlap with merged one\n",
    "        - update pair locations with new token, remove old one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "191b9d68-4451-4399-80a9-44e9a8807dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class BPETokenizerParams:\n",
    "    \"\"\"All you need to specify a BPETokenizer.\"\"\"\n",
    "    vocab: dict[int, bytes]            # index -> bytes\n",
    "    merges: list[tuple[bytes, bytes]]  # index1,index2 -> new_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3457556a-f094-456a-bcda-45e18f5d2f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_bytes(bytes_tuple: tuple[bytes]) -> Iterator[bytes]:\n",
    "    return map(lambda x: bytes([x]) if isinstance(x, int) else bytes(x), bytes_tuple)\n",
    "\n",
    "\n",
    "def merge_bytes(bytes_tuple: tuple[bytes], sep=b'') -> bytes:\n",
    "    return sep.join(to_bytes(bytes_tuple))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ba671a9-1395-4ed6-9f99-e66913095110",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_debug_msg(msg: str):\n",
    "    print()\n",
    "    print('='*60)\n",
    "    print(msg)\n",
    "    print()\n",
    "\n",
    "\n",
    "def print_debug_structs(\n",
    "    freq_table: dict[tuple[bytes], int],\n",
    "    pair_counts: dict[tuple[bytes], int],\n",
    "    pair_locations: dict[tuple[bytes], set[tuple[bytes]]],\n",
    "):\n",
    "    # print frequency table\n",
    "    print('freq_table:')\n",
    "    _t_freq_table_cnt = {tuple(to_bytes(pair)): cnt for pair, cnt in freq_table.items()}\n",
    "    print(_t_freq_table_cnt)\n",
    "    print()\n",
    "    # print pair count\n",
    "    print('pair_counts:')\n",
    "    _t_pair_cnt = {merge_bytes(pair, b\"|\"): cnt for pair, cnt in pair_counts.items()}\n",
    "    print(_t_pair_cnt)\n",
    "    print()\n",
    "    # print pair locations\n",
    "    print('pair_locations:')\n",
    "    _t_pair_locations_f = {\n",
    "        merge_bytes(pair, b\"|\"): [tuple(to_bytes(loc)) for loc in locs]\n",
    "        for pair, locs in pair_locations.items()\n",
    "    }\n",
    "    print(f'{_t_pair_locations_f}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fdf9b689-ab7b-4669-8a54-ce7bbefaa737",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bpe_optimized_debug(\n",
    "    vocab_src: dict[int, bytes],\n",
    "    freq_table_src: dict[tuple[bytes], int],\n",
    "    num_merges: int,\n",
    "    vocab_size: int = 10_000,\n",
    ") -> BPETokenizerParams:\n",
    "    \"\"\"Optimized BPE training with incremental pair frequency updates.\n",
    "\n",
    "    This is the implementation for debug purposes.\n",
    "    \n",
    "    Args:\n",
    "        vocab_src: Initial vocabulary\n",
    "        freq_table_src: Pre-tokenized vocabulary with frequencies: {(b'l',b'o',b'w'): 5, ...}\n",
    "        num_merges: Number of merge operations to perform\n",
    "        \n",
    "    Returns:\n",
    "        BPETokenizerParams consisting of learned vocab and list of merge operations\n",
    "            in order they were learned\n",
    "    \"\"\"\n",
    "    vocab = vocab_src.copy()\n",
    "    freq_table = freq_table_src.copy()\n",
    "    # Init data structures\n",
    "    merges = []\n",
    "    pair_counts = defaultdict(int)\n",
    "    pair_locations = defaultdict(set)\n",
    "    # Precompute all initial pairs and their locations\n",
    "    for token, freq in freq_table.items():\n",
    "        for pair in zip(token, token[1:]):\n",
    "            pair_counts[pair] += freq\n",
    "            pair_locations[pair].add(token)\n",
    "\n",
    "    for merge_idx in range(num_merges):\n",
    "        \n",
    "        if len(pair_counts) == 0 or len(vocab) >= vocab_size:\n",
    "            # stop if all possible pairs were merged or met vocab_size threshold\n",
    "            msgs = ['All possible pairs were merged', 'Met vocab_size threshold']\n",
    "            msg = msgs[0] if len(pair_counts) == 0 else msgs[1]\n",
    "            print(f'\\n>>> {msg}')\n",
    "            break\n",
    "        \n",
    "        print_debug_msg(f'>>> Merge ({merge_idx+1})')\n",
    "        print_debug_structs(freq_table, pair_counts, pair_locations)\n",
    "        \n",
    "        # TODO: Could use heap instead of max\n",
    "        pair_to_merge = max(pair_counts.items(), key=lambda p_cnt: (p_cnt[1], merge_bytes(p_cnt[0], b\"|\")))[0]\n",
    "\n",
    "        print(f'pair_to_merge: {merge_bytes(pair_to_merge, b\"|\")}')\n",
    "    \n",
    "        # Update structures\n",
    "        merges.append(tuple(to_bytes(pair_to_merge)))\n",
    "        merged = merge_bytes(pair_to_merge)\n",
    "        vocab[len(vocab)] = merged\n",
    "        first, second = pair_to_merge\n",
    "        print(f'merged_pair: {merged}')\n",
    "    \n",
    "        print()\n",
    "        \n",
    "        # Update frequency table and tracking structures\n",
    "        affected_tokens = pair_locations[pair_to_merge].copy()\n",
    "        pairs_to_remove = set()  # Non existing pairs after merge\n",
    "        for old_token in affected_tokens:\n",
    "            print(f'old_token: {tuple(to_bytes(old_token))}')\n",
    "            # Merge and obtain new_token\n",
    "            new_token = []\n",
    "            pi = 0\n",
    "            while pi < len(old_token):\n",
    "                if pi < len(old_token)-1 and old_token[pi] == first and old_token[pi+1] == second:\n",
    "                    new_token.append(merged)\n",
    "                    pi += 2\n",
    "                else:\n",
    "                    new_token.append(old_token[pi])\n",
    "                    pi += 1\n",
    "\n",
    "            new_token = tuple(new_token)\n",
    "            print(f'  new_token: {tuple(to_bytes(new_token))}')\n",
    "            \n",
    "            # Initialize frequency table for new token\n",
    "            freq_table[new_token] = freq_table[old_token]\n",
    "            # Remove old token from frequency table\n",
    "            del freq_table[old_token]\n",
    "\n",
    "            # Update pairs_count and pair_locations for new token\n",
    "            for new_pair in zip(new_token, new_token[1:]):\n",
    "                if new_pair[0] == merged and new_pair[1] == merged:\n",
    "                    # merged pairs are joint: (re, re)\n",
    "                    rem_pair = (second, first)\n",
    "                elif new_pair[0] == merged or new_pair[1] == merged:\n",
    "                    # pairs that intersect with merged: (re, a) | (a, re)\n",
    "                    rem_pair = (second, new_pair[1]) if new_pair[0] == merged else (new_pair[0], first)\n",
    "                else:\n",
    "                    # old pairs that don't intersect with merged: (a, b)\n",
    "                    rem_pair = new_pair\n",
    "                pair_counts[rem_pair] -= freq_table[new_token]\n",
    "                pair_counts[new_pair] += freq_table[new_token]\n",
    "                if pair_counts[rem_pair] == 0:\n",
    "                    pairs_to_remove.add(rem_pair)\n",
    "                pair_locations[new_pair].add(new_token)\n",
    "                if old_token in pair_locations[new_pair]:\n",
    "                    # basically \"if old pair\"\n",
    "                    pair_locations[new_pair].remove(old_token)\n",
    "                if old_token in pair_locations[rem_pair]:\n",
    "                    pair_locations[rem_pair].remove(old_token)\n",
    "\n",
    "        # Remove non existing pairs\n",
    "        for rem_pair in pairs_to_remove:\n",
    "            del pair_counts[rem_pair]\n",
    "            del pair_locations[rem_pair]\n",
    "        del pair_counts[pair_to_merge]\n",
    "        del pair_locations[pair_to_merge]\n",
    "    \n",
    "    res = BPETokenizerParams(vocab, merges)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "add292cc-7704-4b84-8eb3-04ab7719b636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ">>> All possible pairs were merged\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"\"\"\n",
    "low low low low low\n",
    "lower lower widest widest widest\n",
    "newest newest newest newest newest newest\n",
    "ererer\n",
    "\"\"\".replace('\\n', ' ').strip()\n",
    "\n",
    "t_res_dummy = pretokenize_dummy(sample_text)\n",
    "\n",
    "# _t_bpe_res = train_bpe_optimized_debug(vocabulary, t_res_dummy, num_merges=20)\n",
    "_t_bpe_res = custom_bpe.train_bpe_optimized(vocabulary, t_res_dummy, num_merges=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ead897c1-6603-4553-85bd-35ffbca216ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256: b'<|endoftext|>'\n",
      "257: b'st'\n",
      "258: b'est'\n",
      "259: b'ow'\n",
      "260: b'low'\n",
      "261: b'west'\n",
      "262: b'ne'\n",
      "263: b'newest'\n",
      "264: b'er'\n",
      "265: b'wi'\n",
      "266: b'wid'\n",
      "267: b'widest'\n",
      "268: b'lower'\n",
      "269: b'erer'\n",
      "270: b'ererer'\n"
     ]
    }
   ],
   "source": [
    "for idx in range(256, 280):\n",
    "    if idx in _t_bpe_res.vocab:\n",
    "        print(f'{idx}: {_t_bpe_res.vocab[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0f71beb8-bbe3-4828-8855-ec2384faebe4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b's', b't'),\n",
       " (b'e', b'st'),\n",
       " (b'o', b'w'),\n",
       " (b'l', b'ow'),\n",
       " (b'w', b'est'),\n",
       " (b'n', b'e'),\n",
       " (b'ne', b'west'),\n",
       " (b'e', b'r'),\n",
       " (b'w', b'i'),\n",
       " (b'wi', b'd'),\n",
       " (b'wid', b'est'),\n",
       " (b'low', b'er'),\n",
       " (b'er', b'er'),\n",
       " (b'erer', b'er')]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_t_bpe_res.merges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8244bd2-4f59-406f-a221-92f6bb59a8db",
   "metadata": {},
   "source": [
    "## 2.4. Encoding-Decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "98eca212-efa8-4559-9567-588d7cbddebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_pretokenize(text: str, pat: str) -> list[tuple[bytes]]:\n",
    "    tokens = []\n",
    "    for match in re.finditer(pat, text):\n",
    "        token = match.group()\n",
    "        token_bytes = tuple(to_bytes(token.encode(\"utf-8\")))\n",
    "        tokens.append(token_bytes)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "95a8727e-e1e7-4683-ae3b-c91799a936a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge(token_src: tuple[bytes], merges: list[tuple[bytes, bytes]]) -> tuple[bytes]:\n",
    "    token = token_src\n",
    "    for first, second in merges:\n",
    "        new_token = []\n",
    "        merged = merge_bytes(to_bytes((first, second)))\n",
    "        pi = 0\n",
    "        while pi < len(token):\n",
    "            if pi < len(token)-1 and token[pi] == first and token[pi+1] == second:\n",
    "                new_token.append(merged)\n",
    "                pi += 2\n",
    "            else:\n",
    "                new_token.append(token[pi])\n",
    "                pi += 1\n",
    "        token = tuple(new_token)\n",
    "        if len(token) == 1:\n",
    "            break\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "e62545ef-5018-4105-b3d5-f68aa6271b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizerCustom:\n",
    "    def __init__(self, params: BPETokenizerParams):\n",
    "        self.params = params\n",
    "        \n",
    "    def encode(self, string: str) -> list[int]:\n",
    "        pre_tokens = encoding_pretokenize(sample_text, PRETOKEN_PAT)\n",
    "        encoding = []\n",
    "        for pre_tok in pre_tokens:\n",
    "            merged_bytes = merge(pre_tok, self.params.merges)\n",
    "            token_encoding = list(map(sample_vocab_inv.get, merged_bytes))\n",
    "            encoding.extend(token_encoding)\n",
    "        return encoding\n",
    "        \n",
    "    def decode(self, indices: list[int]) -> str:\n",
    "        merged_bytes = merge_bytes(map(self.params.vocab.get, indices))\n",
    "        string = merged_bytes.decode(\"utf-8\", errors='replace')\n",
    "        return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f9f46bb5-eb04-4693-9299-02ebe127ec60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_vocab = {\n",
    "    0: b' ', 1: b'a', 2: b'c', 3: b'e', 4: b'h', 5: b't',\n",
    "    6: b'th', 7: b' c', 8: b' a', 9: b'the', 10: b' at',\n",
    "}\n",
    "\n",
    "sample_vocab_inv = {byte: idx for idx, byte in sample_vocab.items()}\n",
    "\n",
    "sample_merges = [\n",
    "    (b't', b'h'), (b' ', b'c'), (b' ', b'a'),\n",
    "    (b'th', b'e'), (b' a', b't'),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d5b53740-ae83-44c5-b58a-5f680cee567c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 7, 1, 5, 10, 3]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_text = \"the cat ate\"\n",
    "\n",
    "bpe_encode_params = BPETokenizerParams(vocab=sample_vocab, merges=sample_merges)\n",
    "bpe_encode = BPETokenizerCustom(bpe_encode_params)\n",
    "\n",
    "_t_enc = bpe_encode.encode(sample_text)\n",
    "_t_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9b15cc46-d7c3-4f91-b3a9-a9075ffce988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the cat ate'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_t_dec = bpe_encode.decode(_t_enc)\n",
    "_t_dec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b766d3db-a3d8-47c7-b3d1-f506a4fdd25d",
   "metadata": {},
   "source": [
    "## 2.5. Assignment Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c669b93b-22f0-4ce7-b389-5142d20513a5",
   "metadata": {},
   "source": [
    "To test your BPE training function against our provided tests, you will first need to implement the test adapter at `[adapters.run_train_bpe]`. Then, run `uv run pytest tests/test_train_bpe.py`. Your implementation should be able to pass all tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "690e4a1b-5ff0-4c8c-9ccc-63a15bdccee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_ROOT_P = '/mnt/data/DatasetsML/NLP/natural_language_corpus/tiny_stories'\n",
    "\n",
    "TRAIN_P = os.path.join(DATA_ROOT_P, 'TinyStoriesV2-GPT4-train.txt')\n",
    "VALID_P = os.path.join(DATA_ROOT_P, 'TinyStoriesV2-GPT4-valid.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c9d17f4a-edc5-422f-b504-8c8235ee3599",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_PROC = 8\n",
    "TOKEN_SPLIT = \"<|endoftext|>\".encode(\"utf-8\")\n",
    "PRETOKEN_PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "\n",
    "N_MAX_MERGES = 10_000\n",
    "\n",
    "\n",
    "@timer\n",
    "def train_bpe(input_path: str, vocab_size: int, special_tokens: list[str]):\n",
    "    # 1. Vocabulary initialization\n",
    "    vocabulary = init_vocabulary(special_tokens)\n",
    "    # 2. Pre-tokenization\n",
    "    pre_tok_params = PreTokenizerArgs(N_PROC, TOKEN_SPLIT, special_tokens, PRETOKEN_PAT)\n",
    "    pre_tok_res = pretokenize_file_parallel(VALID_P, pre_tok_params)\n",
    "    # 3. Compute BPE merges / Train BPE\n",
    "    bpe_trained_res = custom_bpe.train_bpe_optimized(vocabulary, pre_tok_res, N_MAX_MERGES)\n",
    "    # bpe_trained_res = train_bpe_optimized_debug(vocabulary, pre_tok_res, N_MAX_MERGES)\n",
    "    return bpe_trained_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b0f302a4-9944-4a68-bd11-36a0d720da44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 'train_bpe' executed in 5.884 seconds\n"
     ]
    }
   ],
   "source": [
    "N_MAX_VOCAB = 10_000\n",
    "SPECIAL_TOKENS = ['<|endoftext|>']\n",
    "\n",
    "N_MAX_MERGES = 1000\n",
    "\n",
    "train_bpe_res = train_bpe(VALID_P, N_MAX_VOCAB, SPECIAL_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "67f74ebd-3280-4919-90da-d10bf10cac7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_bpe_res.merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7134d3b1-4a92-4502-ad83-41715716c710",
   "metadata": {},
   "source": [
    "# 3. Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e9f48-2d41-4043-9d96-9e5280fb5b4e",
   "metadata": {},
   "source": [
    "## 3.1. BPE Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "90b99ef2-f768-4a5b-82b3-262b4770ecf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6f79106c-c172-4d42-b0cf-fd17cb0ccf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "low low low low low\n",
    "lower lower widest widest widest\n",
    "newest newest newest newest newest newest\n",
    "start restart false\n",
    "\"\"\".replace('\\n', ' ').strip()\n",
    "\n",
    "t_pretok_dummy = pretokenize_dummy(sample_text)\n",
    "\n",
    "_t_bpe_res = train_bpe_optimized(vocabulary, t_pretok_dummy, num_merges=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b38141e5-9c71-4855-b07d-e06f1830f107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256: b'<|endoftext|>'\n",
      "257: b'st'\n",
      "258: b'est'\n",
      "259: b'ow'\n",
      "260: b'low'\n",
      "261: b'west'\n",
      "262: b'ne'\n",
      "263: b'newest'\n",
      "264: b'wi'\n",
      "265: b'wid'\n",
      "266: b'widest'\n",
      "267: b'rt'\n",
      "268: b'lowe'\n",
      "269: b'lower'\n"
     ]
    }
   ],
   "source": [
    "for idx in range(256, 300):\n",
    "    if idx in _t_bpe_res.vocab:\n",
    "        print(f'{idx}: {_t_bpe_res.vocab[idx]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "582ef7ed-ec87-43c3-8d89-a5c71aab287d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(b's', b't'),\n",
       " (b'e', b'st'),\n",
       " (b'o', b'w'),\n",
       " (b'l', b'ow'),\n",
       " (b'w', b'est'),\n",
       " (b'n', b'e'),\n",
       " (b'ne', b'west'),\n",
       " (b'w', b'i'),\n",
       " (b'wi', b'd'),\n",
       " (b'wid', b'est'),\n",
       " (b'r', b't'),\n",
       " (b'low', b'e'),\n",
       " (b'lowe', b'r')]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_t_bpe_res.merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d421a811-660c-47c9-a84f-45d27710443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tokenizatoin/sample_text to test tokenizer\n",
    "target_text = \"The new model features the lowest price and the widest selection of colors in its category.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
